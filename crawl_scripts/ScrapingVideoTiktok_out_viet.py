# %%
# ===========================
# 0) C√ÄI TH∆Ø VI·ªÜN C·∫¶N THI·∫æT
# ===========================
!pip -q install yt-dlp requests pandas xlsxwriter
print("‚úÖ ƒê√£ c√†i ƒë·∫∑t xong c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt.")


# %%
# ===========================
# 1) KHAI B√ÅO & MOUNT DRIVE (tu·ª≥ ch·ªçn)
# ===========================
# from google.colab import drive
# drive.mount('/content/drive')
# BASE_DRIVE_DIR = "/content/drive/MyDrive/SE363/Crawl"   # ƒë·ªïi n·∫øu mu·ªën l∆∞u th·∫≥ng v√†o Drive c·ªßa b·∫°n
# print("Output folder:", BASE_DRIVE_DIR)

import os

ROOT_DIR = (
    os.path.dirname(os.path.abspath(__file__))
    if "__file__" in globals()
    else os.getcwd()
)
# 2. ƒê·ªãnh nghƒ©a c√°c ƒë∆∞·ªùng d·∫´n con
CRAWL_DIR = os.path.join(ROOT_DIR, "data_viet", "crawl")
VIDEO_DIR = os.path.join(ROOT_DIR, "data_viet", "videos")

# ƒê∆∞·ªùng d·∫´n file CSV ƒë·∫ßu v√†o (ƒë∆∞·ª£c t·∫°o b·ªüi script crawl)
INPUT_CSV = os.path.join(CRAWL_DIR, "sub_tiktok_links_viet.csv")
HARMFUL_DIR = os.path.join(VIDEO_DIR, "harmful")
NOT_HARMFUL_DIR = os.path.join(VIDEO_DIR, "not_harmful")

# 3. T·∫°o c√°c th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
os.makedirs(CRAWL_DIR, exist_ok=True)
os.makedirs(VIDEO_DIR, exist_ok=True)
os.makedirs(HARMFUL_DIR, exist_ok=True)
os.makedirs(NOT_HARMFUL_DIR, exist_ok=True)


print(f"üìÇ Th∆∞ m·ª•c d·ª± √°n: {ROOT_DIR}")
print(f"üìÑ File d·ªØ li·ªáu ƒë·∫ßu v√†o: {INPUT_CSV}")
print(f"üìÅ Th∆∞ m·ª•c l∆∞u video HARMFUL: {HARMFUL_DIR}")
print(f"üìÅ Th∆∞ m·ª•c l∆∞u video NOT_HARMFUL: {NOT_HARMFUL_DIR}")


# %% [markdown]
# # Step up crawl video id follow dataframe index

# %%
# START_INDEX = 0 # BAn
# END_INDEX = 332 # BAn

START_INDEX = 0 # Khoi
END_INDEX = 1000 # Khoi

# START_INDEX = 666 # Nam
# END_INDEX = 999 # Nam


# %%
# ===========================
# 2) IMPORT & N·∫†P COOKIES.TXT (Netscape)
#    -> KH√îNG g√°n th·∫≥ng v√†o header "Cookie"
#    -> D√πng MozillaCookieJar + requests.Session
# ===========================
import os, re, json, time, requests
import http.cookiejar as cookielib
from urllib.parse import urlparse
from datetime import datetime, timezone, timedelta
import pandas as pd
import yt_dlp

COOKIES_PATH = "cookies.txt"  # Upload file n√†y l√™n Colab c√πng notebook

# Ki·ªÉm tra cookies.txt t·ªìn t·∫°i
if not os.path.exists(COOKIES_PATH):
    from google.colab import files
    print("Vui l√≤ng upload cookies.txt (Export t·ª´ tr√¨nh duy·ªát).")
    uploaded = files.upload()
    COOKIES_PATH = next(iter(uploaded))

# T·∫°o session d√πng cookie Netscape
cj = cookielib.MozillaCookieJar()
cj.load(COOKIES_PATH, ignore_discard=True, ignore_expires=True)

session = requests.Session()
session.cookies = cj
session.headers.update({
    "User-Agent": "Mozilla/5.0",
    "Referer": "https://www.tiktok.com/"
})

# Ki·ªÉm tra c√≥ cookie cho tiktok kh√¥ng
has_tiktok_cookies = any("tiktok.com" in c.domain for c in session.cookies)
print("Cookies for tiktok.com found:", has_tiktok_cookies)


# %%
# ===========================
# 3) H√ÄM H·ªñ TR·ª¢ (oEmbed, parse URL/aweme_id)
# ===========================
def fetch_oembed(video_url, timeout=12):
    """L·∫•y metadata oEmbed c∆° b·∫£n c·ªßa video (official)."""
    oembed_url = "https://www.tiktok.com/oembed"
    try:
        r = session.get(oembed_url, params={"url": video_url}, timeout=timeout)
        if r.status_code == 200:
            return r.json()
        return None
    except Exception as e:
        print("oEmbed error:", e)
        return None

def extract_aweme_id_from_url(video_url: str):
    """Parse aweme_id t·ª´ URL d·∫°ng /video/<digits>"""
    m = re.search(r"/video/(\d+)", video_url)
    return m.group(1) if m else None


# %%
# ===========================
# 4) DOWNLOAD VIDEO B·∫∞NG yt-dlp (d√πng cookiefile + headers)
# ===========================
def download_tiktok_with_yt_dlp(video_url, out_dir, write_info_json=True, max_filesize=None):
    os.makedirs(out_dir, exist_ok=True)
    out_template = os.path.join(out_dir, "%(id)s.%(ext)s")

    ydl_opts = {
        "outtmpl": out_template,
        "format": "best",
        "noplaylist": True,
        # D√πng cookiefile + UA/Referer ƒë·ªÉ gi·∫£m l·ªói regional/age/403
        "cookiefile": COOKIES_PATH,
        "http_headers": {
            "User-Agent": "Mozilla/5.0",
            "Referer": video_url
        },
        # Tr√°nh t·∫£i l·∫°i c√πng video
        "download_archive": os.path.join(out_dir, "downloaded.txt"),
    }

    if write_info_json:
        ydl_opts.update({
            "writedescription": True,
            "writesubtitles": False,
            "writeinfojson": True
        })

    if max_filesize:
        ydl_opts["max_filesize"] = max_filesize

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(video_url, download=True)

    # L·∫•y info json n·∫øu c√≥
    info_json_path = None
    if isinstance(info, dict) and "id" in info:
        candidate = os.path.join(out_dir, f"{info['id']}.info.json")
        if os.path.exists(candidate):
            info_json_path = candidate

    if info_json_path:
        with open(info_json_path, "r", encoding="utf-8") as f:
            info_data = json.load(f)
    else:
        info_data = info
    return info_data


# %%
# ===========================
# 5) CRAWL COMMENT (WEB API KH√îNG CH√çNH TH·ª®C) B·∫∞NG session
#    L∆∞u √Ω: C√≥ th·ªÉ 403/empty tr√™n Colab d√π c√≥ cookies.
# ===========================
def fetch_comments_web(aweme_id: str, session: requests.Session, max_comments=200, sleep=1.0):
    url = "https://www.tiktok.com/api/comment/list/"
    cursor, out = 0, []
    while len(out) < max_comments:
        params = {"aid": 1988, "aweme_id": aweme_id, "cursor": cursor, "count": 20}
        r = session.get(url, params=params, timeout=20)
        if r.status_code != 200:
            print("HTTP", r.status_code, r.text[:200])
            break
        try:
            data = r.json()
        except Exception as e:
            print("JSON parse error:", e, r.text[:200]); break

        out.extend(data.get("comments") or [])
        print(f"Fetched {len(out)} comments so far‚Ä¶")

        if not data.get("has_more"):
            break
        cursor = data.get("cursor", cursor + 20)
        if sleep: time.sleep(sleep)
    return out

def fetch_replies_web(aweme_id: str, comment_id: str, session: requests.Session, max_replies=100, sleep=1.0):
    url = "https://www.tiktok.com/api/comment/list/reply/"
    cursor, out = 0, []
    while len(out) < max_replies:
        params = {"aid": 1988, "aweme_id": aweme_id, "comment_id": comment_id, "cursor": cursor, "count": 20}
        r = session.get(url, params=params, timeout=20)
        if r.status_code != 200:
            print("HTTP", r.status_code, r.text[:200])
            break
        try:
            data = r.json()
        except Exception as e:
            print("JSON parse error:", e, r.text[:200]); break

        out.extend(data.get("comments") or [])
        if not data.get("has_more"):
            break
        cursor = data.get("cursor", cursor + 20)
        if sleep: time.sleep(sleep)
    return out

def fetch_all_comments_with_replies_web(aweme_id: str, session: requests.Session,
                                        max_comments=200, max_replies_per_comment=50, sleep=1.0):
    comments = fetch_comments_web(aweme_id, session, max_comments=max_comments, sleep=sleep)
    results = []
    for c in comments:
        item = {
            "cid": c.get("cid"),
            "text": c.get("text"),
            "author": (c.get("user") or {}).get("nickname"),
            "create_time": c.get("create_time"),
            "like_count": c.get("digg_count"),
            "reply_count": c.get("reply_comment_total"),
            "replies": []
        }
        try:
            if item["cid"]:
                rs = fetch_replies_web(aweme_id, item["cid"], session, max_replies=max_replies_per_comment, sleep=sleep)
                item["replies"] = rs
        except Exception as e:
            print(f"‚ö†Ô∏è Reply error for {item['cid']}: {e}")
        results.append(item)
    return results


# %%
# ===========================
# 6) H√ÄM CRAWL 1 VIDEO: oEmbed ‚Üí yt-dlp ‚Üí comments ‚Üí JSON
#    + Fallback l·∫•y aweme_id n·∫øu oEmbed kh√¥ng c√≥
# ===========================
def crawl_one_tiktok(video_url, out_dir, use_comments=False, max_comments=200, max_replies_per_comment=50, sleep=1.0):
    os.makedirs(out_dir, exist_ok=True)
    result = {
        "url": video_url,
        "crawled_at": datetime.utcnow().isoformat() + "Z"
    }

    # 1) oEmbed
    oembed = fetch_oembed(video_url)
    result["oembed"] = oembed

    # 2) T·∫£i video + ƒë·ªçc info
    try:
        meta = download_tiktok_with_yt_dlp(video_url, out_dir)
        result["yt_dlp_info"] = meta
    except Exception as e:
        result["yt_dlp_error"] = str(e)
        meta = None

    # 3) L·∫•y aweme_id
    aweme_id = None
    if oembed and isinstance(oembed, dict):
        aweme_id = oembed.get("embed_product_id")
    if not aweme_id:
        aweme_id = extract_aweme_id_from_url(video_url)
    if not aweme_id and isinstance(meta, dict):
        aweme_id = str(meta.get("id"))

    if not aweme_id:
        raise ValueError("Kh√¥ng l·∫•y ƒë∆∞·ª£c aweme_id cho video n√†y")

    print("aweme_id:", aweme_id)

    # 4) Comments (tu·ª≥ ch·ªçn)
    if use_comments:
        cmts = fetch_all_comments_with_replies_web(
            aweme_id, session, max_comments=max_comments,
            max_replies_per_comment=max_replies_per_comment, sleep=sleep
        )
        result["comments"] = cmts

    # 5) Save metadata JSON
    json_path = os.path.join(out_dir, f"{aweme_id}_crawl.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    return result, json_path


# %%
# ===========================
# 7) XU·∫§T EXCEL (comments + replies th√†nh 1 sheet)
# ===========================
# def export_comments_to_excel(all_items, aweme_id: str, out_dir: str):
#     rows = []
#     for c in all_items or []:
#         # Top-level
#         rows.append({
#             "video_id": aweme_id,
#             "cid": c.get("cid"),
#             "parent_cid": None,
#             "is_reply": 0,
#             "author_name": c.get("author"),
#             "text": c.get("text"),
#             "like_count": c.get("like_count"),
#             "reply_count": c.get("reply_count"),
#             "create_time": c.get("create_time"),
#         })
#         # Replies
#         for r in (c.get("replies") or []):
#             ru = r.get("user") or {}
#             rows.append({
#                 "video_id": aweme_id,
#                 "cid": r.get("cid"),
#                 "parent_cid": c.get("cid"),
#                 "is_reply": 1,
#                 "author_name": ru.get("nickname"),
#                 "text": r.get("text"),
#                 "like_count": r.get("digg_count") or r.get("like_count"),
#                 "reply_count": r.get("reply_comment_total") or r.get("reply_count"),
#                 "create_time": r.get("create_time"),
#             })

#     df = pd.DataFrame(rows)

#     # Epoch -> th·ªùi gian
#     if "create_time" in df.columns:
#         dt = pd.to_datetime(df["create_time"], unit="s", errors="coerce", utc=True)
#         df["created_at_utc"] = dt
#         df["created_at_vn"] = dt.dt.tz_convert("Asia/Ho_Chi_Minh")

#     # S·∫Øp c·ªôt
#     cols = ["video_id","cid","parent_cid","is_reply","author_name",
#             "text","like_count","reply_count","create_time",
#             "created_at_utc","created_at_vn"]
#     df = df[[c for c in cols if c in df.columns]]

#     os.makedirs(out_dir, exist_ok=True)
#     xlsx_path = os.path.join(out_dir, f"{aweme_id}_comments.xlsx")
#     with pd.ExcelWriter(xlsx_path, engine="xlsxwriter") as writer:
#         df.to_excel(writer, index=False, sheet_name="comments")
#         ws = writer.sheets["comments"]
#         ws.freeze_panes(1, 0)
#         for i, col in enumerate(df.columns):
#             maxlen = min(60, max(10, df[col].astype(str).str.len().max() if not df.empty else 10))
#             ws.set_column(i, i, maxlen + 2)
#     print("‚úÖ Saved Excel:", xlsx_path)
#     return xlsx_path, df


# %%


def export_comments_to_excel(all_items, aweme_id: str, out_dir: str):
    import os, pandas as pd
    os.makedirs(out_dir, exist_ok=True)

    rows = []
    for c in all_items or []:
        # Top-level
        rows.append({
            "video_id": aweme_id,
            "cid": c.get("cid"),
            "parent_cid": None,
            "is_reply": 0,
            "author_name": c.get("author"),
            "text": c.get("text"),
            "like_count": c.get("like_count"),
            "reply_count": c.get("reply_count"),
            "create_time": c.get("create_time"),
        })
        # Replies
        for r in (c.get("replies") or []):
            ru = r.get("user") or {}
            rows.append({
                "video_id": aweme_id,
                "cid": r.get("cid"),
                "parent_cid": c.get("cid"),
                "is_reply": 1,
                "author_name": ru.get("nickname"),
                "text": r.get("text"),
                "like_count": r.get("digg_count") or r.get("like_count"),
                "reply_count": r.get("reply_comment_total") or r.get("reply_count"),
                "create_time": r.get("create_time"),
            })

    df = pd.DataFrame(rows)

    # Epoch -> datetime (tz-aware), r·ªìi b·ªè tz ƒë·ªÉ Excel ch·∫•p nh·∫≠n
    if "create_time" in df.columns:
        dt_utc = pd.to_datetime(df["create_time"], unit="s", errors="coerce", utc=True)
        # B·∫¢N NAIVE (kh√¥ng timezone)
        df["created_at_utc"] = dt_utc.dt.tz_localize(None)
        df["created_at_vn"]  = dt_utc.dt.tz_convert("Asia/Ho_Chi_Minh").dt.tz_localize(None)

    # S·∫Øp c·ªôt
    cols = ["video_id","cid","parent_cid","is_reply","author_name",
            "text","like_count","reply_count","create_time",
            "created_at_utc","created_at_vn"]
    df = df[[c for c in cols if c in df.columns]]

    xlsx_path = os.path.join(out_dir, f"{aweme_id}_comments.xlsx")
    with pd.ExcelWriter(xlsx_path, engine="xlsxwriter",
                        datetime_format="yyyy-mm-dd hh:mm:ss") as writer:
        df.to_excel(writer, index=False, sheet_name="comments")
        ws = writer.sheets["comments"]
        ws.freeze_panes(1, 0)
        # Auto-width
        for i, col in enumerate(df.columns):
            maxlen = min(60, max(10, df[col].astype(str).str.len().max() if not df.empty else 10))
            ws.set_column(i, i, maxlen + 2)

    print("‚úÖ Saved Excel:", xlsx_path)
    return xlsx_path, df



# %%
# ===========================
# 8) CH·∫†Y T·ª∞ ƒê·ªòNG T·ª™ FILE CSV (C√ì PH√ÇN LO·∫†I & RESUME)
# ===========================
import pandas as pd
import time
import os
from tqdm.notebook import tqdm # B·ªè comment n·∫øu mu·ªën d√πng thanh ti·∫øn tr√¨nh ƒë·∫πp h∆°n

# 1. ƒê·ªçc v√† L·ªçc file CSV
if not os.path.exists(INPUT_CSV):
    print(f"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file {INPUT_CSV}")
    print("Vui l√≤ng ch·∫°y script 'find_tiktok_links.py' tr∆∞·ªõc ƒë·ªÉ c√≥ d·ªØ li·ªáu.")
else:
    print(f"ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´: {INPUT_CSV} ...")
    df = pd.read_csv(INPUT_CSV)
    print(f"-> T·ªïng s·ªë d√≤ng trong CSV: {len(df)}")

    # L·ªåC QUAN TR·ªåNG: Ch·ªâ l·∫•y nh·ªØng link l√† VIDEO (c√≥ ch·ª©a '/video/')
    # Lo·∫°i b·ªè c√°c link profile (v√≠ d·ª•: tiktok.com/@username)
    # L√†m s·∫°ch link (b·ªè c√°c ph·∫ßn th·ª´a sau d·∫•u ?)
    # Lo·∫°i b·ªè c√°c link tr√πng l·∫∑p
    df_videos = df[df['link'].str.contains('/video/', na=False)].copy()
    df_videos['link'] = df_videos['link'].apply(lambda x: x.split('?')[0])
    df_videos = df_videos.drop_duplicates(subset=['link'])
    print(f"-> S·ªë l∆∞·ª£ng VIDEO th·ª±c t·∫ø c·∫ßn crawl: {len(df_videos)}")
    
    # L·ªçc dataframe theo kho·∫£ng index
    df_videos = df_videos.iloc[START_INDEX:END_INDEX]

    print(f"-> S·ªë l∆∞·ª£ng VIDEO th·ª±c t·∫ø c·∫ßn crawl theo index: {len(df_videos)}")
    print("=======================================================")
    

    # 2. V√≤ng l·∫∑p Crawl ch√≠nh
    # (D√πng itertuples ƒë·ªÉ l·∫∑p qua dataframe nhanh h∆°n)
    # (D√πng tqdm b·ªçc df_videos.itertuples() ƒë·ªÉ c√≥ thanh ti·∫øn tr√¨nh)
    for row in tqdm(df_videos.itertuples(index=True), total=len(df_videos), desc="T·ªïng ti·∫øn ƒë·ªô"):
        index = row.Index  # index dataframe
        original_url = row.link
        label = row.label
        hashtag = row.hashtag
        
        # a) L·∫•y ID video
        aweme_id = extract_aweme_id_from_url(original_url)
        if not aweme_id:
            print(f"\n   ‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c ID video t·ª´ URL: {original_url}. B·ªè qua.")
            continue

        # --- T√™n th∆∞ m·ª•c theo index dataframe ---
        if label == 'harmful':
            base_save_dir = HARMFUL_DIR
        else:
            base_save_dir = NOT_HARMFUL_DIR

        folder_name = f"video_{index}"

        print(f"\n‚ñ∂Ô∏è ƒêang x·ª≠ l√Ω: {original_url}")
        print(f"   üè∑Ô∏è Nh√£n: {label.upper()} | #Ô∏è‚É£ Hashtag: #{hashtag}")
        print(f"   üìÅ T√™n th∆∞ m·ª•c: {folder_name} (ID: {aweme_id})")

        # c) T·∫°o th∆∞ m·ª•c ri√™ng cho video n√†y
        video_specific_dir = os.path.join(base_save_dir, folder_name)

        # d) Ki·ªÉm tra RESUME
        if os.path.exists(video_specific_dir) and os.listdir(video_specific_dir):
            print(f"   ‚è≠Ô∏è Th∆∞ m·ª•c '{folder_name}' ƒë√£ t·ªìn t·∫°i d·ªØ li·ªáu. B·ªè qua (Resume).")
            continue

        os.makedirs(video_specific_dir, exist_ok=True)

        # e) B·∫Øt ƒë·∫ßu Crawl (G·ªçi l·∫°i h√†m crawl_one_tiktok c·ªßa b·∫°n)
        try:
            print(f"   ‚¨áÔ∏è B·∫Øt ƒë·∫ßu crawl v√†o: {video_specific_dir} ...")

            # --- G·ªåI H√ÄM CRAWL G·ªêC ---
            res, jsonp = crawl_one_tiktok(
                original_url,
                video_specific_dir,
                use_comments=True,          # L·∫•y comment
                max_comments=200,           # TƒÉng s·ªë l∆∞·ª£ng n·∫øu c·∫ßn
                max_replies_per_comment=50,
                sleep=1.0                   # Ngh·ªâ gi·ªØa c√°c l·∫ßn g·ªçi API comment
            )
            print(f"   ‚úÖ Metadata JSON saved: {os.path.basename(jsonp)}")

            # --- XU·∫§T EXCEL COMMENT ---
            cmts = res.get("comments", [])
            if cmts:
                # G·ªçi h√†m export_comments_to_excel g·ªëc
                xlsx_path, _ = export_comments_to_excel(cmts, aweme_id, video_specific_dir)
                if xlsx_path:
                    print(f"   ‚úÖ Comments Excel saved: {os.path.basename(xlsx_path)} ({len(cmts)} cmts)")
            else:
                print("   ‚ÑπÔ∏è Kh√¥ng t√¨m th·∫•y comment n√†o.")

        except Exception as e:
             print(f"   ‚ùå L·ªñI KHI CRAWL VIDEO N√ÄY: {e}")
             # C√≥ th·ªÉ x√≥a th∆∞ m·ª•c l·ªói n·∫øu mu·ªën s·∫°ch s·∫Ω
             # import shutil
             # shutil.rmtree(video_specific_dir, ignore_errors=True)

        # f) Ngh·ªâ ng∆°i ƒë·ªÉ tr√°nh b·ªã ch·∫∑n (Rate Limit)
        print("   üí§ ƒêang ngh·ªâ 5s...")
        time.sleep(5)

    print("\nüéâüéâüéâ ƒê√É HO√ÄN T·∫§T TO√ÄN B·ªò DANH S√ÅCH VIDEO! üéâüéâüéâ")


# %%
# # ===========================
# # 8) CH·∫†Y T·ª∞ ƒê·ªòNG T·ª™ FILE TXT (C√ì C∆† CH·∫æ RESUME)
# # ===========================
# import time
# import os # ƒê·∫£m b·∫£o os ƒë√£ ƒë∆∞·ª£c import (m·∫∑c d√π ƒë√£ c√≥ ·ªü √¥ 3)

# URL_FILE_PATH = "urls.txt" # T√™n file b·∫°n ƒë√£ upload

# # --- ƒê·ªçc file urls.txt ---
# try:
#     with open(URL_FILE_PATH, 'r') as f:
#         VIDEO_URL_LIST = [line.strip() for line in f if line.strip() and line.strip().startswith("http")]
    
#     if not VIDEO_URL_LIST:
#         print(f"L·ªói: File {URL_FILE_PATH} tr·ªëng ho·∫∑c kh√¥ng ch·ª©a link h·ª£p l·ªá.")
#     else:
#         print(f"*** ƒê√£ t√¨m th·∫•y {len(VIDEO_URL_LIST)} URLs trong file. B·∫Øt ƒë·∫ßu crawl... ***")

# except FileNotFoundError:
#     print(f"L·ªói: Kh√¥ng t√¨m th·∫•y file {URL_FILE_PATH}. B·∫°n ƒë√£ upload file ch∆∞a?")
#     VIDEO_URL_LIST = []
# # -----------------------------


# # D√πng enumerate ƒë·ªÉ l·∫•y c·∫£ index (0, 1, 2...) v√† url
# for index, url in enumerate(VIDEO_URL_LIST):
    
#     # T·∫°o t√™n th∆∞ m·ª•c, v√≠ d·ª•: "vid_1", "vid_2"...
#     folder_name = f"vid_{index + 1}"
    
#     # T·∫°o ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß ƒë·∫øn th∆∞ m·ª•c m·ªõi n√†y
#     video_specific_dir = os.path.join(CRAWL_DIR, folder_name)
    
#     # -------- C∆† CH·∫æ RESUME (PH·∫¶N TH√äM M·ªöI) --------
#     # Ki·ªÉm tra xem th∆∞ m·ª•c n√†y ƒë√£ t·ªìn t·∫°i hay ch∆∞a
#     if os.path.exists(video_specific_dir):
#         # N·∫øu ƒë√£ t·ªìn t·∫°i, in th√¥ng b√°o v√† b·ªè qua (continue)
#         print(f"\n--- [B·ªé QUA] Th∆∞ m·ª•c '{folder_name}' ƒë√£ t·ªìn t·∫°i. Chuy·ªÉn sang video ti·∫øp theo. ---")
#         continue # L·ªánh m·∫•u ch·ªët: D·ª´ng v√≤ng l·∫∑p n√†y, ƒëi ƒë·∫øn video ti·∫øp theo
#     # -----------------------------------------------

#     # N·∫øu th∆∞ m·ª•c ch∆∞a t·ªìn t·∫°i, T·∫†O M·ªöI v√† b·∫Øt ƒë·∫ßu crawl
#     os.makedirs(video_specific_dir, exist_ok=True) 

#     print(f"\n=======================================================")
#     print(f"--- [B·∫ÆT ƒê·∫¶U] X·ª≠ l√Ω: {url} (L∆∞u v√†o: {video_specific_dir}) ---")
    
#     try:
#         # G·ªçi h√†m crawl v√† TRUY·ªÄN TH∆Ø M·ª§C M·ªöI v√†o:
#         res, jsonp = crawl_one_tiktok(
#             url, video_specific_dir, 
#             use_comments=True,         
#             max_comments=100,          
#             max_replies_per_comment=50 
#         )
#         print(f"‚úÖ ƒê√£ l∆∞u Metadata JSON v√†o: {jsonp}")

#         # N·∫øu c√≥ comments th√¨ xu·∫•t Excel
#         cmts = res.get("comments", [])
#         if cmts:
#             aweme_id = extract_aweme_id_from_url(url) or (res.get("oembed") or {}).get("embed_product_id")
#             if aweme_id:
#                 xlsx_path, df_preview = export_comments_to_excel(cmts, aweme_id, video_specific_dir) 
#                 print(f"‚úÖ ƒê√£ l∆∞u Comments Excel v√†o: {xlsx_path}")
#             else:
#                  print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y aweme_id, b·ªè qua xu·∫•t Excel.")
#         else:
#             print("‚ÑπÔ∏è Video n√†y kh√¥ng c√≥ comment (ho·∫∑c crawl comment th·∫•t b·∫°i).")

#     except Exception as e:
#         print(f"!!!!!!!! L·ªñI NGHI√äM TR·ªåNG !!!!!!!!")
#         print(f"L·ªói khi x·ª≠ l√Ω {url}: {e}")
#         print(f"--- [B·ªé QUA] Video n√†y v√† ti·∫øp t·ª•c. ---")

#     print(f"=======================================================")
    
#     time.sleep(5)

# print("\n*** ƒê√É HO√ÄN T·∫§T TO√ÄN B·ªò DANH S√ÅCH! ***")



