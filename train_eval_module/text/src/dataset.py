"""
Text Dataset - Phi√™n b·∫£n ƒë∆°n gi·∫£n, chu·∫©n HuggingFace.

Input: CSV v·ªõi c·ªôt text (chu·ªói text d√†i, ƒë√£ g·ªôp comments b·∫±ng [SEP])
Output: Dict chu·∫©n cho Trainer {input_ids, attention_mask, labels}
"""

import torch
import re
import unicodedata
from torch.utils.data import Dataset
import pandas as pd
import os
import sys

sys.path.append(
    os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
)
from configs.paths import TEXT_TRAIN_CSV, TEXT_VAL_CSV, TEXT_TEST_CSV


def clean_text(text):
    """
    L√†m s·∫°ch text TikTok ti·∫øng Vi·ªát tr∆∞·ªõc khi tokenize.
    Gi·ªØ nguy√™n [SEP] ƒë·ªÉ model hi·ªÉu ranh gi·ªõi gi·ªØa c√°c comment.
    """
    if not isinstance(text, str):
        return ""

    if not text.strip():
        return ""

    # 1. Unicode Normalize - Chu·∫©n h√≥a ti·∫øng Vi·ªát
    text = unicodedata.normalize("NFC", text)

    # 2. X√≥a zero-width characters
    text = re.sub(r"[\u200b\u200c\u200d\ufeff\u00ad]", "", text)

    # 3. X·ª≠ l√Ω URLs - Thay b·∫±ng token ƒë·∫∑c bi·ªát
    text = re.sub(r"https?://\S+|www\.\S+", " ", text)

    # 4. X·ª≠ l√Ω mentions v√† hashtags
    text = re.sub(r"@[\w\.]+", " ", text)  # @username
    text = re.sub(r"#(\w+)", r" \1 ", text)  # #hashtag -> hashtag

    # 5. X·ª≠ l√Ω emoji - Gi·ªØ l·∫°i emoji v√¨ ch√∫ng mang √Ω nghƒ©a
    # Kh√¥ng x√≥a emoji, BERT tokenizer s·∫Ω x·ª≠ l√Ω

    # 6. X·ª≠ l√Ω k√Ω t·ª± l·∫∑p (v√≠ d·ª•: "ƒë·∫πpppp" -> "ƒë·∫πpp")
    text = re.sub(r"(.)\1{2,}", r"\1\1", text)

    # 7. X·ª≠ l√Ω d·∫•u c√¢u l·∫∑p
    text = re.sub(r"\.{2,}", ".", text)
    text = re.sub(r"!{2,}", "!", text)
    text = re.sub(r"\?{2,}", "?", text)

    # 8. X·ª≠ l√Ω s·ªë d√†i (s·ªë ƒëi·ªán tho·∫°i, ID)
    text = re.sub(r"\b\d{7,}\b", " ", text)

    # 9. Chu·∫©n h√≥a kho·∫£ng tr·∫Øng quanh [SEP]
    text = re.sub(r"\s*\[SEP\]\s*", " [SEP] ", text)

    # 10. X·ª≠ l√Ω c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát kh√¥ng c·∫ßn thi·∫øt
    text = re.sub(r"[\_\-\=\+\*\~\`\|\\\<\>]+", " ", text)

    # 11. Chu·∫©n h√≥a kho·∫£ng tr·∫Øng cu·ªëi c√πng
    text = re.sub(r"\s+", " ", text).strip()

    # 12. X·ª≠ l√Ω text qu√° ng·∫Øn ho·∫∑c ch·ªâ c√≥ [SEP]
    cleaned = text.replace("[SEP]", "").strip()
    if len(cleaned) < 2:
        return ""

    return text


def load_text_data(split="train"):
    """Load tr·ª±c ti·∫øp t·ª´ c√°c file CSV ƒë√£ split s·∫µn."""
    csv_file = None
    if split == "train":
        csv_file = TEXT_TRAIN_CSV
    elif split == "val":
        csv_file = TEXT_VAL_CSV
    elif split == "test":
        csv_file = TEXT_TEST_CSV

    if not csv_file or not os.path.exists(csv_file):
        print(f"‚ùå [Text] Kh√¥ng t√¨m th·∫•y file split: {csv_file}")
        return pd.DataFrame()

    print(f"üìÇ [Text] Loading {split.upper()} from: {csv_file}")
    df = pd.read_csv(csv_file)

    # ƒê·∫£m b·∫£o format chu·∫©n
    df = df.dropna(subset=["text", "label"])
    df = df.reset_index(drop=True)
    df["label"] = df["label"].astype(int)
    df["text"] = df["text"].astype(str)

    return df


class TextDataset(Dataset):
    """
    Dataset ƒë∆°n gi·∫£n cho text classification.
    Input text ƒë√£ l√† chu·ªói d√†i (comments g·ªôp b·∫±ng [SEP]).
    """

    def __init__(self, df, tokenizer, max_len=512):
        self.df = df
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        text = str(row["text"])
        label = int(row["label"])

        # Clean text
        text = clean_text(text)

        # Tokenize - Tokenizer s·∫Ω t·ª± truncate
        enc = self.tokenizer(
            text,
            padding="max_length",
            truncation=True,
            max_length=self.max_len,
            return_tensors="pt",
        )

        return {
            "input_ids": enc["input_ids"].squeeze(0),  # (L,)
            "attention_mask": enc["attention_mask"].squeeze(0),  # (L,)
            "labels": torch.tensor(label, dtype=torch.long),
        }
