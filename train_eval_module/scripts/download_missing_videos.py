"""Download TikTok videos that exist in text labels but are missing on disk.

Why this exists
--------------
For multimodal training, splits are done by `video_id` at the video level.
If the text label CSV contains `video_id`s that do not exist as video files
under the configured raw video folders, those text rows must be dropped to
avoid split leakage / misalignment.

This script uses:
- processed_data/text/missing_text_video_ids.txt  (generated by split_data.py)
- processed_data/text/TRAINING_TEXT_DATA_FINAL_COMBINED.csv (to infer label 0/1)
- data*/crawl/*.csv files containing TikTok links (to resolve video_id -> URL)

It downloads missing videos into:
  <BASE_PROJECT_PATH>/data/videos/{harmful|not_harmful}/missing/

After download, re-run:
  python scripts/split_data.py

Notes
-----
- Requires yt-dlp installed in the active env.
- If you have a cookies.txt at repo root, it will be used automatically.

"""

from __future__ import annotations

import argparse
import os
import re
from typing import Dict, Iterable, List, Optional, Set, Tuple

import pandas as pd

try:
    import yt_dlp
except ImportError as e:
    raise ImportError(
        "yt-dlp is required. Install it in your environment (e.g., pip install yt-dlp)."
    ) from e

# Setup import path for configs
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from configs.paths import BASE_PROJECT_PATH


VIDEO_ID_RE = re.compile(r"/video/(\d+)")


def _read_lines(path: str) -> List[str]:
    if not os.path.exists(path):
        return []
    with open(path, "r", encoding="utf-8") as f:
        return [ln.strip() for ln in f if ln.strip()]


def load_missing_ids(path: str) -> Set[str]:
    return set(_read_lines(path))


def build_url_map(crawl_csv_paths: Iterable[str]) -> Dict[str, str]:
    """Map video_id -> canonical URL from crawl CSV files."""
    out: Dict[str, str] = {}
    for fp in crawl_csv_paths:
        if not os.path.exists(fp):
            continue
        try:
            df = pd.read_csv(fp)
        except Exception:
            continue
        if "link" not in df.columns:
            continue
        for link in df["link"].dropna().astype(str).tolist():
            m = VIDEO_ID_RE.search(link)
            if not m:
                continue
            vid = m.group(1)
            # Keep first seen URL (deterministic) but strip query params
            if vid not in out:
                out[vid] = link.split("?")[0]
    return out


def infer_label_map_from_text_csv(text_csv_path: str) -> Dict[str, int]:
    """Infer 0/1 label for each video_id from the text label CSV.

    If multiple rows exist for the same video_id, prefer highest confidence if available.
    """
    df = pd.read_csv(text_csv_path)
    df = df.dropna(subset=["video_id", "label"]).copy()
    df["video_id"] = df["video_id"].astype(str)

    # normalize label
    if df["label"].dtype == object:
        label_map = {
            "harmful": 1,
            "not_harmful": 0,
            "safe": 0,
            "0": 0,
            "1": 1,
        }
        df["label"] = df["label"].astype(str).str.strip().str.lower().map(label_map)
    df = df.dropna(subset=["label"]).copy()
    df["label"] = df["label"].astype(int)

    if "confidence" in df.columns:
        df["confidence"] = pd.to_numeric(df["confidence"], errors="coerce")
        df = df.sort_values(by=["confidence"], ascending=False)

    # pick first per video_id after sorting
    df = df.drop_duplicates(subset=["video_id"], keep="first")
    return dict(zip(df["video_id"].tolist(), df["label"].tolist()))


def ensure_dir(p: str) -> None:
    os.makedirs(p, exist_ok=True)


def download_one(url: str, out_dir: str, cookies_path: Optional[str]) -> None:
    ensure_dir(out_dir)

    out_template = os.path.join(out_dir, "%(id)s.%(ext)s")
    archive_path = os.path.join(out_dir, "downloaded.txt")

    ydl_opts = {
        "outtmpl": out_template,
        "format": "best",
        "noplaylist": True,
        "retries": 3,
        "fragment_retries": 3,
        "concurrent_fragment_downloads": 4,
        "download_archive": archive_path,
        "quiet": True,
        "no_warnings": True,
    }

    if cookies_path and os.path.exists(cookies_path):
        ydl_opts["cookiefile"] = cookies_path
        ydl_opts["http_headers"] = {
            "User-Agent": "Mozilla/5.0",
            "Referer": url,
        }

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        ydl.extract_info(url, download=True)


def main(
    limit: Optional[int],
    dry_run: bool,
    data_root: str,
) -> None:
    # Inputs
    missing_ids_path = os.path.join(
        BASE_PROJECT_PATH, "processed_data", "text", "missing_text_video_ids.txt"
    )
    text_csv_path = os.path.join(
        BASE_PROJECT_PATH,
        "processed_data",
        "text",
        "TRAINING_TEXT_DATA_FINAL_COMBINED.csv",
    )

    missing_ids = load_missing_ids(missing_ids_path)
    if not missing_ids:
        print(f"âœ… No missing ids found at: {missing_ids_path}")
        return

    # Crawl sources
    crawl_csv_paths = [
        os.path.join(BASE_PROJECT_PATH, "data", "crawl", "tiktok_links.csv"),
        os.path.join(BASE_PROJECT_PATH, "data", "crawl", "sub_tiktok_links.csv"),
        os.path.join(BASE_PROJECT_PATH, "data", "crawl", "sub_tiktok_links_1.csv"),
        os.path.join(BASE_PROJECT_PATH, "data_1", "crawl", "sub_tiktok_links_1.csv"),
        os.path.join(
            BASE_PROJECT_PATH, "data_viet", "crawl", "sub_tiktok_links_viet.csv"
        ),
    ]

    url_map = build_url_map(crawl_csv_paths)
    label_map = infer_label_map_from_text_csv(text_csv_path)

    # Output folders
    data_root = data_root.strip()
    if data_root not in {"data", "data_1", "data_viet"}:
        raise ValueError("data_root must be one of: data, data_1, data_viet")

    harmful_dir = os.path.join(
        BASE_PROJECT_PATH, data_root, "videos", "harmful", "missing"
    )
    safe_dir = os.path.join(
        BASE_PROJECT_PATH, data_root, "videos", "not_harmful", "missing"
    )
    ensure_dir(harmful_dir)
    ensure_dir(safe_dir)

    cookies_path = os.path.join(BASE_PROJECT_PATH, "cookies.txt")
    if os.path.exists(cookies_path):
        print(f"ðŸª Using cookies: {cookies_path}")
    else:
        cookies_path = None
        print(
            "ðŸª No cookies.txt found at repo root. Downloads may fail due to 403/region."
        )

    # Determine worklist
    work: List[Tuple[str, str]] = []
    missing_no_url = 0
    missing_no_label = 0
    for vid in sorted(missing_ids):
        url = url_map.get(vid)
        if not url:
            missing_no_url += 1
            continue
        y = label_map.get(vid)
        if y is None:
            missing_no_label += 1
            continue
        work.append((vid, url))

    print(f"Missing ids: {len(missing_ids)}")
    print(f"Resolvable (have url+label): {len(work)}")
    print(f"Missing url: {missing_no_url} | Missing label: {missing_no_label}")

    if dry_run:
        print("\n[dry-run] First 10 downloads:")
        for vid, url in work[:10]:
            y = label_map.get(vid)
            out_dir = harmful_dir if int(y) == 1 else safe_dir
            print(f"  - {vid} -> {url} -> {out_dir}")
        return

    if limit is not None:
        work = work[: max(0, int(limit))]

    ok = 0
    fail = 0
    for i, (vid, url) in enumerate(work, start=1):
        y = label_map.get(vid)
        out_dir = harmful_dir if int(y) == 1 else safe_dir
        try:
            download_one(url, out_dir=out_dir, cookies_path=cookies_path)
            ok += 1
            if i % 25 == 0:
                print(f"Downloaded {ok}/{i} (fail={fail})")
        except Exception as e:
            fail += 1
            print(f"[FAIL] {vid} {url} -> {e}")

    print(f"âœ… Done. ok={ok}, fail={fail}")
    print("Next: run `python scripts/split_data.py` to regenerate splits.")


if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--limit", type=int, default=None, help="download at most N videos")
    ap.add_argument("--dry-run", action="store_true", help="print plan only")
    ap.add_argument(
        "--data-root",
        type=str,
        default="data",
        help="which raw data folder to store downloads into: data|data_1|data_viet",
    )
    args = ap.parse_args()

    main(limit=args.limit, dry_run=args.dry_run, data_root=args.data_root)
