from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, udf, struct, when, lit
from pyspark.sql.types import StructType, StructField, StringType, FloatType, DoubleType
from pyspark import StorageLevel
import boto3
import os
import tempfile
import torch
import numpy as np
import scipy.io.wavfile as wavfile
import re
import psycopg2
from psycopg2.extras import execute_values
from datetime import datetime

# --- HUGGING FACE IMPORTS ---
from transformers import AutoImageProcessor, VideoMAEForVideoClassification
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import AutoFeatureExtractor, AutoModelForAudioClassification
from decord import VideoReader, cpu


# --- Cáº¤U HÃŒNH ---
KAFKA_BOOTSTRAP_SERVERS = "kafka:29092"
KAFKA_TOPIC = "tiktok_raw_data"

# Kafka start offset (default: latest Ä‘á»ƒ trÃ¡nh reprocess khi restart)
KAFKA_STARTING_OFFSETS = os.getenv("KAFKA_STARTING_OFFSETS", "latest")

# Spark checkpoint (persist Ä‘á»ƒ trÃ¡nh Ä‘á»c láº¡i dá»¯ liá»‡u khi restart container)
SPARK_CHECKPOINT_DIR = os.getenv(
    "SPARK_CHECKPOINT_DIR", "/opt/spark/checkpoints/tiktok_multimodal"
)

# Tuning (cho phÃ©p test thá»§ cÃ´ng qua env, khÃ´ng cáº§n sá»­a code)
TEXT_WEIGHT = float(os.getenv("TEXT_WEIGHT", "0.3"))
TEXT_WEIGHT = max(0.0, min(1.0, TEXT_WEIGHT))
VIDEO_WEIGHT = 1.0 - TEXT_WEIGHT
DECISION_THRESHOLD = float(os.getenv("DECISION_THRESHOLD", "0.5"))
DECISION_THRESHOLD = max(0.0, min(1.0, DECISION_THRESHOLD))

# NOTE: Ä‘á»c tá»« env Ä‘á»ƒ Ä‘á»“ng bá»™ vá»›i docker-compose/.env
MINIO_ENDPOINT = os.getenv("MINIO_ENDPOINT", "http://minio:9000")
MINIO_ACCESS_KEY = os.getenv("MINIO_ROOT_USER", "admin")
MINIO_SECRET_KEY = os.getenv("MINIO_ROOT_PASSWORD", "password123")

POSTGRES_HOST = os.getenv("POSTGRES_HOST", "postgres")
POSTGRES_PORT = os.getenv("POSTGRES_PORT", "5432")
POSTGRES_DB = os.getenv("POSTGRES_DB", "tiktok_safety_db")
POSTGRES_USER = os.getenv("POSTGRES_USER", "user")
POSTGRES_PASSWORD = os.getenv("POSTGRES_PASSWORD", "password")

DAG_ID = "2_TIKTOK_STREAMING_PIPELINE"
TASK_NAME = "spark_processor"


def log_to_db(message, level="INFO"):
    """Ghi log ra stdout + ghi vÃ o Postgres (báº£ng system_logs) Ä‘á»ƒ Dashboard hiá»ƒn thá»‹."""
    ts = datetime.utcnow().isoformat(timespec="seconds")
    print(f"[{ts}] [{level}] {message}", flush=True)
    try:
        conn = psycopg2.connect(
            dbname=POSTGRES_DB,
            user=POSTGRES_USER,
            password=POSTGRES_PASSWORD,
            host=POSTGRES_HOST,
            port=POSTGRES_PORT,
        )
        cur = conn.cursor()
        cur.execute(
            "INSERT INTO system_logs (dag_id, task_name, log_level, message) VALUES (%s, %s, %s, %s)",
            (DAG_ID, TASK_NAME, level, message),
        )
        conn.commit()
        conn.close()
    except Exception as e:
        # KhÃ´ng lÃ m fail streaming job chá»‰ vÃ¬ lá»—i ghi log
        print(f"[{ts}] [WARN] log_to_db failed: {e}", flush=True)


# --- PATH MODELS (ÄÃ£ Mount) ---
PATH_TEXT_MODEL = "/models/text/output/uitnlp_CafeBERT/train/best_checkpoint_FocalLoss"
PATH_VIDEO_MODEL = "/models/video/output/MCG-NJU_videomae-base-finetuned-kinetics/train/best_checkpoint"
PATH_AUDIO_MODEL = "/models/audio/audio_model/checkpoint-2300"

TEXT_LABEL_MAP = {0: "safe", 1: "harmful"}
VIDEO_LABEL_MAP = {0: "safe", 1: "harmful"}

# --- [Äá»’NG Bá»˜] BLACKLIST KEYWORDS (Dá»±a trÃªn RISKY_HASHTAGS cá»§a Crawler) ---
BLACKLIST_KEYWORDS = [
    # 1. NhÃ³m GÃ¡i xinh / Sexy / 18+
    "gaixinh",
    "gÃ¡i xinh",
    "nháº£y sexy",
    "nhay sexy",
    "khoe body",
    "khoe dÃ¡ng",
    "bikini",
    "há»Ÿ báº¡o",
    "sugar baby",
    "sugarbaby",
    "sgbb",
    "nuÃ´i baby",
    "phÃ²ng the",
    "phong the",
    "chuyá»‡n ngÆ°á»i lá»›n",
    "18+",
    "lá»™ clip",
    "khoe hÃ ng",
    # 2. NhÃ³m Báº¡o lá»±c / Drama / Giang há»“
    "Ä‘Ã¡nh nhau",
    "danh nhau",
    "Ä‘Ã¡nh ghen",
    "danh ghen",
    "bÃ³c phá»‘t",
    "boc phot",
    "drama",
    "showbiz",
    "xÄƒm trá»•",
    "giang há»“",
    "biáº¿n cÄƒng",
    "check var",
    "há»—n chiáº¿n",
    "báº¡o lá»±c há»c Ä‘Æ°á»ng",
    "chá»­i báº­y",
    # 3. NhÃ³m Cá» báº¡c / Lá»«a Ä‘áº£o / TÃ i chÃ­nh Ä‘en
    "tÃ i xá»‰u",
    "xÃ³c Ä‘Ä©a",
    "xoc dia",
    "ná»• hÅ©",
    "no hu",
    "báº¯n cÃ¡",
    "soi kÃ¨o",
    "cho vay",
    "bá»‘c bÃ¡t há»",
    "kiáº¿m tiá»n online",
    "lá»«a Ä‘áº£o",
    "app vay tiá»n",
    "nhÃ³m kÃ©o",
    "kÃ©o tÃ i xá»‰u",
    "cÃ¡ Ä‘á»™",
    "lÃ´ Ä‘á»",
    # 4. NhÃ³m Tá»‡ náº¡n / Cháº¥t kÃ­ch thÃ­ch
    "bay láº¯c",
    "dÃ¢n chÆ¡i",
    "trÃ  Ä‘Ã¡ vá»‰a hÃ¨",
    "nháº­u nháº¹t",
    "say rÆ°á»£u",
    "hÃºt thuá»‘c",
    "vape",
    "pod",
    "cáº§n sa",
    "ke",
    "káº¹o",
    # 5. NhÃ³m TÃ¢m linh / MÃª tÃ­n
    "gá»i vong",
    "xem bÃ³i",
    "bÃ¹a ngáº£i",
    "kumathong",
    "kumanthong",
    "tÃ¢m linh",
]

# --- GLOBAL VARS ---
text_tokenizer = None
text_model = None
video_processor = None
video_model = None
audio_extractor = None
audio_model = None
device = "cpu"


# --- LAZY LOADING FUNCTIONS ---
def get_text_model():
    global text_tokenizer, text_model
    if text_model is None:
        print(f"ðŸ“¦ Loading Text Model...")
        text_tokenizer = AutoTokenizer.from_pretrained(PATH_TEXT_MODEL)
        text_model = AutoModelForSequenceClassification.from_pretrained(PATH_TEXT_MODEL)
        text_model.to(device)
        text_model.eval()
    return text_tokenizer, text_model


def get_video_model():
    global video_processor, video_model
    if video_model is None:
        print(f"ðŸ“¦ Loading Video Model...")
        video_processor = AutoImageProcessor.from_pretrained(PATH_VIDEO_MODEL)
        video_model = VideoMAEForVideoClassification.from_pretrained(PATH_VIDEO_MODEL)
        video_model.to(device)
        video_model.eval()
    return video_processor, video_model


def get_audio_model():
    global audio_extractor, audio_model
    if audio_model is None:
        print(f"ðŸ“¦ Loading Audio Model: {PATH_AUDIO_MODEL}")
        audio_extractor = AutoFeatureExtractor.from_pretrained(PATH_AUDIO_MODEL)
        audio_model = AutoModelForAudioClassification.from_pretrained(PATH_AUDIO_MODEL)
        audio_model.to(device)
        audio_model.eval()
    return audio_extractor, audio_model


# --- UDF VIDEO ---
def process_video_logic(video_id, minio_path):
    temp_file = None
    try:
        if not minio_path:
            return {"risk_score": 0.0, "verdict": "NoVideo", "status": "Skip"}

        s3 = boto3.client(
            "s3",
            endpoint_url=MINIO_ENDPOINT,
            aws_access_key_id=MINIO_ACCESS_KEY,
            aws_secret_access_key=MINIO_SECRET_KEY,
        )
        parts = minio_path.split("/", 1)

        fd, temp_name = tempfile.mkstemp(suffix=".mp4")
        os.close(fd)
        temp_file = temp_name
        s3.download_file(parts[0], parts[1], temp_file)

        vr = VideoReader(temp_file, ctx=cpu(0))
        indices = np.linspace(0, len(vr) - 1, 16).astype(int)
        frames = list(vr.get_batch(indices).asnumpy())

        proc, model = get_video_model()
        inputs = proc(frames, return_tensors="pt").to(device)
        with torch.no_grad():
            outputs = model(**inputs)
            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)

            # Láº¥y score class 1 (Harmful)
            score = probs[0][1].item()
            verdict = "harmful" if score > 0.5 else "safe"

        if os.path.exists(temp_file):
            os.remove(temp_file)
        return {
            "risk_score": float(score),
            "verdict": str(verdict),
            "status": "Success",
        }
    except Exception as e:
        if temp_file and os.path.exists(temp_file):
            os.remove(temp_file)
        return {"risk_score": 0.0, "verdict": "Error", "status": str(e)}


# --- UDF TEXT (RULE-BASED + AI) ---
def process_text_logic(text):
    if not text:
        return {"risk_score": 0.0, "verdict": "Unknown"}

    # 1. RULE-BASED CHECK (Báº¯t dÃ­nh cÃ¡c tá»« khÃ³a tá»« Crawler)
    text_lower = text.lower()
    for kw in BLACKLIST_KEYWORDS:
        if kw in text_lower:
            # Náº¿u dÃ­nh tá»« cáº¥m -> GÃ¡n Ä‘iá»ƒm cao ngay (0.85)
            return {"risk_score": 0.85, "verdict": "harmful"}

    # 2. AI MODEL CHECK (Náº¿u khÃ´ng dÃ­nh tá»« cáº¥m thÃ¬ há»i AI)
    try:
        tok, model = get_text_model()
        inputs = tok(
            text, return_tensors="pt", truncation=True, padding=True, max_length=256
        ).to(device)
        with torch.no_grad():
            outputs = model(**inputs)
            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)

            score = probs[0][1].item()  # Class 1 = Harmful
            verdict = "harmful" if score > 0.5 else "safe"

        return {"risk_score": float(score), "verdict": str(verdict)}
    except Exception as e:
        return {"risk_score": 0.0, "verdict": "Error: " + str(e)}


# --- UDF AUDIO ---
def process_audio_logic(video_id, minio_audio_path):
    # Tráº£ vá» máº·c Ä‘á»‹nh Ä‘á»ƒ trÃ¡nh lá»—i pipeline, sau nÃ y tÃ­ch há»£p model audio sau
    return {"risk_score": 0.0, "verdict": "NoAudio", "status": "Skip"}


# --- REGISTER ---
res_schema = StructType(
    [
        StructField("risk_score", FloatType(), False),
        StructField("verdict", StringType(), False),
        StructField("status", StringType(), False),
    ]
)
text_res_schema = StructType(
    [
        StructField("risk_score", FloatType(), False),
        StructField("verdict", StringType(), False),
    ]
)

process_video_udf = udf(process_video_logic, res_schema)
process_text_udf = udf(process_text_logic, text_res_schema)
process_audio_udf = udf(process_audio_logic, res_schema)


# --- DB WRITER ---
def write_to_postgres(batch_df, batch_id):
    log_to_db(f"--- PROCESSING BATCH {batch_id} ---", "INFO")

    # NOTE:
    # `processed_results` dÃ¹ng `video_id` lÃ m PRIMARY KEY. Khi consumer restart hoáº·c dÃ¹ng startingOffsets=earliest,
    # Spark sáº½ Ä‘á»c láº¡i message -> dá»… bá»‹ duplicate video_id.
    # VÃ¬ váº­y ta UPSERT (ON CONFLICT) Ä‘á»ƒ:
    #  - khÃ´ng crash streaming job
    #  - cáº­p nháº­t `processed_at` Ä‘á»ƒ Dashboard tháº¥y engine váº«n Ä‘ang hoáº¡t Ä‘á»™ng
    cols = [
        "video_id",
        "raw_text",
        "human_label",
        "text_verdict",
        "text_score",
        "video_verdict",
        "video_score",
        "avg_score",
        "threshold",
        "final_decision",
    ]

    # Persist Ä‘á»ƒ trÃ¡nh Spark cháº¡y láº¡i toÃ n bá»™ UDF (download video/model inference) nhiá»u láº§n
    batch_cached = batch_df.select(*cols).persist(StorageLevel.MEMORY_AND_DISK)
    try:
        # Tá»•ng quan batch (Ä‘á»ƒ dá»… hiá»ƒu Ä‘ang streaming nhá»¯ng gÃ¬)
        try:
            total_rows = batch_cached.count()
        except Exception:
            total_rows = None

        try:
            breakdown = (
                batch_cached.groupBy("final_decision")
                .count()
                .toPandas()
                .to_dict("records")
            )
        except Exception:
            breakdown = None

        if total_rows is not None:
            if breakdown is not None:
                log_to_db(
                    f"Batch {batch_id}: rows={total_rows} breakdown={breakdown}",
                    "INFO",
                )
            else:
                log_to_db(f"Batch {batch_id}: rows={total_rows}", "INFO")

        # In sample cáº£ safe/harmful + score Ä‘á»ƒ debug nhanh
        batch_cached.select(
            "video_id",
            "final_decision",
            "avg_score",
            "text_verdict",
            "text_score",
            "video_verdict",
            "video_score",
            "raw_text",
        ).show(8, truncate=True)

        collected = batch_cached.collect()

        # Náº¿u trong 1 micro-batch cÃ³ duplicate video_id (cÃ¹ng PK) thÃ¬ Postgres sáº½ bÃ¡o:
        # "ON CONFLICT DO UPDATE command cannot affect row a second time".
        # Ta de-dup theo video_id, giá»¯ báº£n ghi cuá»‘i cÃ¹ng.
        rows_by_video_id = {}
        for r in collected:
            rows_by_video_id[r["video_id"]] = tuple(r[c] for c in cols)
        rows = list(rows_by_video_id.values())
    except Exception as e:
        log_to_db(f"âŒ Failed collecting batch {batch_id} rows: {e}", "ERROR")
        raise
    finally:
        try:
            batch_cached.unpersist()
        except Exception:
            pass

    if not rows:
        log_to_db(f"â„¹ï¸ Batch {batch_id}: empty (nothing to write)", "INFO")
        return

    upsert_sql = """
        INSERT INTO processed_results
            (video_id, raw_text, human_label, text_verdict, text_score, video_verdict, video_score, avg_score, threshold, final_decision)
        VALUES %s
        ON CONFLICT (video_id) DO UPDATE SET
            raw_text = EXCLUDED.raw_text,
            human_label = EXCLUDED.human_label,
            text_verdict = EXCLUDED.text_verdict,
            text_score = EXCLUDED.text_score,
            video_verdict = EXCLUDED.video_verdict,
            video_score = EXCLUDED.video_score,
            avg_score = EXCLUDED.avg_score,
            threshold = EXCLUDED.threshold,
            final_decision = EXCLUDED.final_decision,
            processed_at = CURRENT_TIMESTAMP
    """

    try:
        conn = psycopg2.connect(
            dbname=POSTGRES_DB,
            user=POSTGRES_USER,
            password=POSTGRES_PASSWORD,
            host=POSTGRES_HOST,
            port=POSTGRES_PORT,
        )
        cur = conn.cursor()
        execute_values(cur, upsert_sql, rows, page_size=100)
        conn.commit()
        conn.close()
    except Exception as e:
        log_to_db(f"âŒ Batch {batch_id}: upsert failed: {e}", "ERROR")
        raise

    log_to_db(f"âœ… Saved Batch {batch_id} | rows={len(rows)}", "INFO")


def main():
    log_to_db("ðŸš€ Spark Streaming Engine starting...", "INFO")
    log_to_db(
        f"Config: startingOffsets={KAFKA_STARTING_OFFSETS}, checkpoint={SPARK_CHECKPOINT_DIR}, w_text={TEXT_WEIGHT:.2f}, w_video={VIDEO_WEIGHT:.2f}, thr={DECISION_THRESHOLD:.2f}",
        "INFO",
    )
    spark = (
        SparkSession.builder.appName("TikTokMultiModalAI")
        .config("spark.sql.streaming.checkpointLocation", SPARK_CHECKPOINT_DIR)
        .config("spark.executor.memory", "8g")
        .config("spark.python.worker.memory", "2g")
        .config("spark.network.timeout", "600s")
        .getOrCreate()
    )
    spark.sparkContext.setLogLevel("ERROR")  # Chá»‰ hiá»‡n lá»—i thá»±c sá»±

    json_schema = StructType(
        [
            StructField("video_id", StringType(), True),
            StructField("minio_video_path", StringType(), True),
            StructField("clean_text", StringType(), True),
            StructField("csv_label", StringType(), True),
            StructField("timestamp", DoubleType(), True),
        ]
    )

    df_kafka = (
        spark.readStream.format("kafka")
        .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS)
        .option("subscribe", KAFKA_TOPIC)
        .option("startingOffsets", KAFKA_STARTING_OFFSETS)
        .option("failOnDataLoss", "false")
        .option("maxOffsetsPerTrigger", 5)
        .load()
    )

    df_parsed = (
        df_kafka.selectExpr("CAST(value AS STRING)")
        .select(from_json(col("value"), json_schema).alias("data"))
        .select("data.*")
    )

    # Cháº¡y AI
    df_analyzed = df_parsed.withColumn(
        "video_ai", process_video_udf(col("video_id"), col("minio_video_path"))
    ).withColumn("text_ai", process_text_udf(col("clean_text")))

    # TÃ­nh Ä‘iá»ƒm: Text 30% + Video 70%
    df_scored = (
        df_analyzed.withColumn("text_score", col("text_ai.risk_score"))
        .withColumn("video_score", col("video_ai.risk_score"))
        .withColumn(
            "avg_score",
            (col("text_score") * lit(TEXT_WEIGHT))
            + (col("video_score") * lit(VIDEO_WEIGHT)),
        )
    )

    df_final = df_scored.select(
        col("video_id"),
        col("clean_text").alias("raw_text"),
        col("csv_label").alias("human_label"),
        col("text_ai.verdict").alias("text_verdict"),
        col("text_score"),
        col("video_ai.verdict").alias("video_verdict"),
        col("video_score"),
        col("avg_score"),
        lit(DECISION_THRESHOLD).alias("threshold"),
        when(col("avg_score") >= lit(DECISION_THRESHOLD), "harmful")
        .otherwise("safe")
        .alias("final_decision"),
    )

    query = df_final.writeStream.foreachBatch(write_to_postgres).start()
    log_to_db("âœ… Spark query started. Waiting for Kafka messages...", "INFO")
    query.awaitTermination()


if __name__ == "__main__":
    main()
